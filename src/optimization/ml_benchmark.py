"""
MLç®—æ³•æ€§èƒ½å¯¹æ¯” - Phase 9.1

å¯¹æ¯”LightGBMã€XGBoostã€RandomForeståœ¨å› å­é¢„æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°
ä½¿ç”¨Walk-Forwardäº¤å‰éªŒè¯,ç¡®ä¿ç»“æœå¯é 
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from loguru import logger
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error
from scipy.stats import ttest_rel
import time


class MLAlgorithmBenchmark:
    """
    MLç®—æ³•æ€§èƒ½å¯¹æ¯”å®éªŒ

    å¯¹æ¯”ç®—æ³•:
    - LightGBM (ä¸»åŠ›)
    - XGBoost (å¯¹ç…§)
    - RandomForest (è¾…åŠ©)
    - Ridge (åŸºå‡†)
    """

    def __init__(self, data: pd.DataFrame, factor_columns: List[str],
                 target_column: str = 'return_5d'):
        """
        Args:
            data: åŒ…å«å› å­å’Œç›®æ ‡çš„DataFrame,æŒ‰æ—¥æœŸç´¢å¼•
            factor_columns: å› å­åˆ—ååˆ—è¡¨
            target_column: ç›®æ ‡åˆ—å(å¦‚'return_5d'è¡¨ç¤ºæœªæ¥5æ—¥æ”¶ç›Š)
        """
        self.data = data
        self.factor_columns = factor_columns
        self.target_column = target_column

        # å‡†å¤‡æ•°æ®
        self.X = data[factor_columns].fillna(0)
        self.y = data[target_column].fillna(0)

        logger.info(f"ML Benchmarkåˆå§‹åŒ–: {len(data)}æ ·æœ¬, {len(factor_columns)}å› å­")

        # å¾…å¯¹æ¯”çš„æ¨¡å‹
        self.models = self._init_models()

    def _init_models(self) -> Dict:
        """åˆå§‹åŒ–å¾…å¯¹æ¯”çš„æ¨¡å‹"""
        models = {}

        # 1. LightGBM
        try:
            import lightgbm as lgb
            models['LightGBM'] = lgb.LGBMRegressor(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.05,
                num_leaves=31,
                min_child_samples=20,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=0.1,
                random_state=42,
                verbose=-1,
            )
            logger.info("âœ… LightGBMå·²åŠ è½½")
        except ImportError:
            logger.warning("âš ï¸  LightGBMæœªå®‰è£…,å°†è·³è¿‡")

        # 2. XGBoost
        try:
            import xgboost as xgb
            models['XGBoost'] = xgb.XGBRegressor(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=0.1,
                random_state=42,
                verbosity=0,
            )
            logger.info("âœ… XGBoostå·²åŠ è½½")
        except ImportError:
            logger.warning("âš ï¸  XGBoostæœªå®‰è£…,å°†è·³è¿‡")

        # 3. RandomForest
        try:
            from sklearn.ensemble import RandomForestRegressor
            models['RandomForest'] = RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                min_samples_split=10,
                min_samples_leaf=5,
                max_features='sqrt',
                random_state=42,
                n_jobs=-1,
            )
            logger.info("âœ… RandomForestå·²åŠ è½½")
        except ImportError:
            logger.warning("âš ï¸  RandomForestæœªå®‰è£…,å°†è·³è¿‡")

        # 4. Ridge (åŸºå‡†)
        try:
            from sklearn.linear_model import Ridge
            models['Ridge'] = Ridge(
                alpha=1.0,
                random_state=42,
            )
            logger.info("âœ… Ridgeå·²åŠ è½½")
        except ImportError:
            logger.warning("âš ï¸  Ridgeæœªå®‰è£…,å°†è·³è¿‡")

        if not models:
            raise ImportError("æ‰€æœ‰MLåº“éƒ½æœªå®‰è£…,æ— æ³•è¿è¡ŒBenchmark")

        return models

    def run_walk_forward_comparison(self, n_splits: int = 5) -> pd.DataFrame:
        """
        Walk-Forwardäº¤å‰éªŒè¯å¯¹æ¯”

        Args:
            n_splits: æ—¶é—´åºåˆ—åˆ’åˆ†æŠ˜æ•°

        Returns:
            å¯¹æ¯”ç»“æœDataFrame
        """
        logger.info(f"å¼€å§‹Walk-Forwardå¯¹æ¯” (n_splits={n_splits})")

        tscv = TimeSeriesSplit(n_splits=n_splits)
        results = {name: [] for name in self.models}

        # æ—¶é—´è®°å½•
        training_times = {name: [] for name in self.models}

        for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(self.X), 1):
            logger.info(f"  Fold {fold_idx}/{n_splits}: è®­ç»ƒ{len(train_idx)}æ ·æœ¬, æµ‹è¯•{len(test_idx)}æ ·æœ¬")

            X_train, X_test = self.X.iloc[train_idx], self.X.iloc[test_idx]
            y_train, y_test = self.y.iloc[train_idx], self.y.iloc[test_idx]

            for name, model in self.models.items():
                # è®­ç»ƒ
                start_time = time.time()
                model.fit(X_train, y_train)
                train_time = time.time() - start_time
                training_times[name].append(train_time)

                # é¢„æµ‹
                y_pred = model.predict(X_test)

                # è¯„ä¼°
                ic = self._calculate_ic(y_test, y_pred)
                rank_ic = self._calculate_rank_ic(y_test, y_pred)
                mse = mean_squared_error(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)

                results[name].append({
                    'fold': fold_idx,
                    'ic': ic,
                    'rank_ic': rank_ic,
                    'mse': mse,
                    'mae': mae,
                    'train_time': train_time,
                })

        # æ±‡æ€»ç»Ÿè®¡
        summary = {}
        for name, scores in results.items():
            ic_values = [s['ic'] for s in scores]
            rank_ic_values = [s['rank_ic'] for s in scores]
            mse_values = [s['mse'] for s in scores]

            summary[name] = {
                'ICå‡å€¼': np.mean(ic_values),
                'ICæ ‡å‡†å·®': np.std(ic_values),
                'Rank_ICå‡å€¼': np.mean(rank_ic_values),
                'Rank_ICæ ‡å‡†å·®': np.std(rank_ic_values),
                'MSEå‡å€¼': np.mean(mse_values),
                'MSEæ ‡å‡†å·®': np.std(mse_values),
                'å¹³å‡è®­ç»ƒæ—¶é—´(ç§’)': np.mean(training_times[name]),
                'è¯¦ç»†ç»“æœ': scores,
            }

        # è½¬ä¸ºDataFrame
        comparison_df = pd.DataFrame(summary).T

        logger.info("Walk-Forwardå¯¹æ¯”å®Œæˆ")
        return comparison_df

    def _calculate_ic(self, y_true: pd.Series, y_pred: np.ndarray) -> float:
        """è®¡ç®—IC (Information Coefficient)"""
        if len(y_true) < 2:
            return 0.0

        # Pearsonç›¸å…³ç³»æ•°
        ic = np.corrcoef(y_true, y_pred)[0, 1]

        return ic if not np.isnan(ic) else 0.0

    def _calculate_rank_ic(self, y_true: pd.Series, y_pred: np.ndarray) -> float:
        """è®¡ç®—Rank IC (æ’åºç›¸å…³ç³»æ•°)"""
        if len(y_true) < 2:
            return 0.0

        # Spearmanç§©ç›¸å…³
        from scipy.stats import spearmanr
        rank_ic, _ = spearmanr(y_true, y_pred)

        return rank_ic if not np.isnan(rank_ic) else 0.0

    def statistical_significance_test(self, comparison_df: pd.DataFrame) -> Dict:
        """
        ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ(t-test)

        æ£€éªŒæœ€ä¼˜ç®—æ³•ä¸å…¶ä»–ç®—æ³•çš„ICå·®å¼‚æ˜¯å¦æ˜¾è‘—

        Args:
            comparison_df: run_walk_forward_comparisonè¿”å›çš„ç»“æœ

        Returns:
            på€¼å­—å…¸
        """
        # æ‰¾å‡ºICå‡å€¼æœ€é«˜çš„ç®—æ³•
        best_model = comparison_df['ICå‡å€¼'].idxmax()
        best_ic_values = [s['ic'] for s in comparison_df.loc[best_model, 'è¯¦ç»†ç»“æœ']]

        logger.info(f"æœ€ä¼˜ç®—æ³•: {best_model} (ICå‡å€¼={comparison_df.loc[best_model, 'ICå‡å€¼']:.4f})")

        p_values = {}
        for name in comparison_df.index:
            if name == best_model:
                continue

            ic_values = [s['ic'] for s in comparison_df.loc[name, 'è¯¦ç»†ç»“æœ']]

            # é…å¯¹tæ£€éªŒ
            t_stat, p_value = ttest_rel(best_ic_values, ic_values)
            p_values[name] = {
                'p_value': p_value,
                'significant': p_value < 0.05,
                't_statistic': t_stat,
            }

            if p_value < 0.05:
                logger.info(f"  {best_model} vs {name}: p={p_value:.4f} (æ˜¾è‘—ä¼˜äº)")
            else:
                logger.info(f"  {best_model} vs {name}: p={p_value:.4f} (æ— æ˜¾è‘—å·®å¼‚)")

        return p_values

    def generate_report(self, comparison_df: pd.DataFrame,
                       p_values: Optional[Dict] = None) -> str:
        """
        ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š

        Args:
            comparison_df: å¯¹æ¯”ç»“æœ
            p_values: ç»Ÿè®¡æ£€éªŒpå€¼(å¯é€‰)

        Returns:
            æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        report = []
        report.append("=" * 70)
        report.append("MLç®—æ³•æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š - Phase 9.1")
        report.append("=" * 70)

        report.append(f"\nğŸ“Š å¯¹æ¯”æ¦‚å†µ:")
        report.append(f"  æ ·æœ¬æ•°: {len(self.data)}")
        report.append(f"  å› å­æ•°: {len(self.factor_columns)}")
        report.append(f"  ç›®æ ‡: {self.target_column}")
        report.append(f"  å¯¹æ¯”ç®—æ³•: {list(comparison_df.index)}")

        report.append(f"\n\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
        report.append("\n" + "-" * 70)
        report.append(f"{'ç®—æ³•':<15} {'ICå‡å€¼':<12} {'ICæ ‡å‡†å·®':<12} {'Rank_ICå‡å€¼':<12} {'è®­ç»ƒæ—¶é—´(s)':<12}")
        report.append("-" * 70)

        for name in comparison_df.index:
            row = comparison_df.loc[name]
            report.append(
                f"{name:<15} "
                f"{row['ICå‡å€¼']:>10.4f}  "
                f"{row['ICæ ‡å‡†å·®']:>10.4f}  "
                f"{row['Rank_ICå‡å€¼']:>12.4f}  "
                f"{row['å¹³å‡è®­ç»ƒæ—¶é—´(ç§’)']:>12.3f}"
            )

        report.append("-" * 70)

        # æ’å
        report.append(f"\n\nğŸ† ICå‡å€¼æ’å:")
        ranked = comparison_df.sort_values('ICå‡å€¼', ascending=False)
        for i, (name, row) in enumerate(ranked.iterrows(), 1):
            medal = {1: 'ğŸ¥‡', 2: 'ğŸ¥ˆ', 3: 'ğŸ¥‰'}.get(i, f'{i}.')
            report.append(f"  {medal} {name}: IC={row['ICå‡å€¼']:.4f} Â± {row['ICæ ‡å‡†å·®']:.4f}")

        # ç»Ÿè®¡æ£€éªŒç»“æœ
        if p_values:
            report.append(f"\n\nğŸ“Š ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ:")
            best_model = ranked.index[0]
            report.append(f"  åŸºå‡†: {best_model}")

            for name, result in p_values.items():
                sig_mark = "âœ… æ˜¾è‘—" if result['significant'] else "âŒ ä¸æ˜¾è‘—"
                report.append(
                    f"  vs {name}: p={result['p_value']:.4f}, t={result['t_statistic']:.2f} ({sig_mark})"
                )

        # å»ºè®®
        report.append(f"\n\nğŸ’¡ å»ºè®®:")
        best_model = ranked.index[0]
        best_ic = ranked.loc[best_model, 'ICå‡å€¼']

        if best_ic > 0.05:
            report.append(f"  âœ… æ¨èä½¿ç”¨ã€{best_model}ã€‘ä½œä¸ºä¸»åŠ›ç®—æ³•")
            report.append(f"     ICå‡å€¼={best_ic:.4f},å…·æœ‰è¾ƒå¼ºé¢„æµ‹èƒ½åŠ›")
        elif best_ic > 0.03:
            report.append(f"  âš ï¸  ã€{best_model}ã€‘å¯ä½œä¸ºè¾…åŠ©ç®—æ³•")
            report.append(f"     ICå‡å€¼={best_ic:.4f},é¢„æµ‹èƒ½åŠ›ä¸­ç­‰")
        else:
            report.append(f"  âŒ æ‰€æœ‰MLç®—æ³•è¡¨ç°ä¸ä½³(æœ€é«˜ICä»…{best_ic:.4f})")
            report.append(f"     å»ºè®®: ä¼˜åŒ–å› å­è´¨é‡æˆ–ä½¿ç”¨çº¯è§„åˆ™ç­–ç•¥")

        report.append("\n" + "=" * 70)

        return "\n".join(report)

    def check_overfitting(self, comparison_df: pd.DataFrame) -> Dict:
        """
        æ£€æŸ¥è¿‡æ‹Ÿåˆæƒ…å†µ

        é€šè¿‡ICæ ‡å‡†å·®åˆ¤æ–­æ¨¡å‹ç¨³å®šæ€§
        æ ‡å‡†å·®è¿‡å¤§è¯´æ˜åœ¨ä¸åŒæ—¶æœŸè¡¨ç°å·®å¼‚å¤§,å¯èƒ½è¿‡æ‹Ÿåˆ

        Args:
            comparison_df: å¯¹æ¯”ç»“æœ

        Returns:
            è¿‡æ‹Ÿåˆæ£€æŸ¥ç»“æœ
        """
        results = {}

        for name in comparison_df.index:
            ic_mean = comparison_df.loc[name, 'ICå‡å€¼']
            ic_std = comparison_df.loc[name, 'ICæ ‡å‡†å·®']

            # ç¨³å®šæ€§å¾—åˆ† = ICå‡å€¼ / ICæ ‡å‡†å·®
            stability_score = abs(ic_mean) / ic_std if ic_std > 0 else 0

            # åˆ¤æ–­
            if ic_std > 0.05:
                status = 'âš ï¸  ä¸ç¨³å®š'
                comment = f'ICæ ‡å‡†å·®è¿‡å¤§({ic_std:.4f}),å¯èƒ½è¿‡æ‹Ÿåˆ'
            elif stability_score > 2.0:
                status = 'âœ… ç¨³å®š'
                comment = f'ç¨³å®šæ€§å¾—åˆ†={stability_score:.2f},è¡¨ç°ç¨³å®š'
            else:
                status = 'ğŸ”¶ ä¸­ç­‰'
                comment = f'ç¨³å®šæ€§å¾—åˆ†={stability_score:.2f}'

            results[name] = {
                'ICå‡å€¼': ic_mean,
                'ICæ ‡å‡†å·®': ic_std,
                'ç¨³å®šæ€§å¾—åˆ†': stability_score,
                'çŠ¶æ€': status,
                'è¯„ä»·': comment,
            }

        return results


def quick_ml_benchmark(data: pd.DataFrame, factor_columns: List[str],
                      target_column: str = 'return_5d', n_splits: int = 5) -> Dict:
    """
    å¿«é€ŸMLç®—æ³•å¯¹æ¯”(ä¾¿æ·å‡½æ•°)

    Args:
        data: æ•°æ®
        factor_columns: å› å­åˆ—è¡¨
        target_column: ç›®æ ‡åˆ—
        n_splits: äº¤å‰éªŒè¯æŠ˜æ•°

    Returns:
        å®Œæ•´å¯¹æ¯”ç»“æœ
    """
    benchmark = MLAlgorithmBenchmark(data, factor_columns, target_column)

    # è¿è¡Œå¯¹æ¯”
    comparison_df = benchmark.run_walk_forward_comparison(n_splits=n_splits)

    # ç»Ÿè®¡æ£€éªŒ
    p_values = benchmark.statistical_significance_test(comparison_df)

    # è¿‡æ‹Ÿåˆæ£€æŸ¥
    overfitting_check = benchmark.check_overfitting(comparison_df)

    # ç”ŸæˆæŠ¥å‘Š
    report = benchmark.generate_report(comparison_df, p_values)

    return {
        'comparison': comparison_df,
        'p_values': p_values,
        'overfitting_check': overfitting_check,
        'report': report,
    }
