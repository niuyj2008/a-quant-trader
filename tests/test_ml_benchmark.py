"""
æµ‹è¯•MLç®—æ³•æ€§èƒ½å¯¹æ¯” - Phase 9.1
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.optimization.ml_benchmark import MLAlgorithmBenchmark, quick_ml_benchmark
import pandas as pd
import numpy as np
from datetime import datetime, timedelta


def create_mock_factor_data(n_samples=500, n_factors=10):
    """
    åˆ›å»ºæ¨¡æ‹Ÿå› å­æ•°æ®(ç”¨äºæµ‹è¯•)

    æ³¨æ„: è¿™æ˜¯æµ‹è¯•ç”¨æ¨¡æ‹Ÿæ•°æ®,ä¸ç”¨äºç”Ÿäº§
    """
    np.random.seed(42)

    # ç”Ÿæˆæ—¥æœŸç´¢å¼•
    dates = pd.date_range(start='2021-01-01', periods=n_samples, freq='D')

    # ç”Ÿæˆå› å­
    factors = {}
    for i in range(n_factors):
        # æ¯ä¸ªå› å­æ˜¯å¸¦è¶‹åŠ¿çš„éšæœºæ¸¸èµ°
        trend = np.linspace(0, 0.1, n_samples)
        noise = np.random.normal(0, 0.05, n_samples)
        factors[f'factor_{i}'] = trend + noise

    # ç”Ÿæˆç›®æ ‡(æœªæ¥5æ—¥æ”¶ç›Š)
    # ä¸å› å­æœ‰ä¸€å®šç›¸å…³æ€§
    weights = np.random.uniform(-0.1, 0.1, n_factors)
    factor_matrix = np.array([factors[f'factor_{i}'] for i in range(n_factors)]).T
    base_return = factor_matrix @ weights

    # æ·»åŠ å™ªå£°
    noise = np.random.normal(0, 0.02, n_samples)
    target = base_return + noise

    # æ„é€ DataFrame
    data = pd.DataFrame(factors, index=dates)
    data['return_5d'] = target

    return data


def test_ml_benchmark_initialization():
    """æµ‹è¯•1: åˆå§‹åŒ–ML Benchmark"""
    print("=" * 60)
    print("æµ‹è¯•1: åˆå§‹åŒ–ML Benchmark")
    print("=" * 60)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    data = create_mock_factor_data(n_samples=500, n_factors=10)
    factor_columns = [col for col in data.columns if col.startswith('factor_')]

    print(f"\næ•°æ®è§„æ¨¡: {len(data)}æ ·æœ¬ Ã— {len(factor_columns)}å› å­")

    # åˆå§‹åŒ–Benchmark
    benchmark = MLAlgorithmBenchmark(
        data=data,
        factor_columns=factor_columns,
        target_column='return_5d'
    )

    print(f"\nå¯ç”¨æ¨¡å‹: {list(benchmark.models.keys())}")

    # éªŒè¯
    assert len(benchmark.models) >= 1, "è‡³å°‘åº”è¯¥åŠ è½½1ä¸ªæ¨¡å‹"
    assert len(benchmark.X) == len(data), "ç‰¹å¾çŸ©é˜µè¡Œæ•°åº”ç­‰äºæ ·æœ¬æ•°"
    assert len(benchmark.y) == len(data), "ç›®æ ‡å‘é‡é•¿åº¦åº”ç­‰äºæ ·æœ¬æ•°"

    print("\nâœ… æµ‹è¯•1é€šè¿‡!")
    return True


def test_walk_forward_comparison():
    """æµ‹è¯•2: Walk-Forwardå¯¹æ¯”"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2: Walk-Forwardå¯¹æ¯”")
    print("=" * 60)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    data = create_mock_factor_data(n_samples=500, n_factors=10)
    factor_columns = [col for col in data.columns if col.startswith('factor_')]

    benchmark = MLAlgorithmBenchmark(data, factor_columns, 'return_5d')

    # è¿è¡Œå¯¹æ¯”(3æŠ˜èŠ‚çœæ—¶é—´)
    print("\nè¿è¡ŒWalk-Forwardå¯¹æ¯”(3æŠ˜)...")
    comparison_df = benchmark.run_walk_forward_comparison(n_splits=3)

    print("\nå¯¹æ¯”ç»“æœ:")
    print(comparison_df[['ICå‡å€¼', 'ICæ ‡å‡†å·®', 'Rank_ICå‡å€¼', 'å¹³å‡è®­ç»ƒæ—¶é—´(ç§’)']])

    # éªŒè¯
    assert len(comparison_df) >= 1, "è‡³å°‘åº”è¯¥æœ‰1ä¸ªæ¨¡å‹çš„ç»“æœ"

    for model_name in comparison_df.index:
        ic_mean = comparison_df.loc[model_name, 'ICå‡å€¼']
        ic_std = comparison_df.loc[model_name, 'ICæ ‡å‡†å·®']

        print(f"\n{model_name}:")
        print(f"  ICå‡å€¼: {ic_mean:.4f}")
        print(f"  ICæ ‡å‡†å·®: {ic_std:.4f}")

        # ICåº”è¯¥åœ¨åˆç†èŒƒå›´
        assert -1 <= ic_mean <= 1, f"{model_name} ICå‡å€¼åº”è¯¥åœ¨[-1,1]"
        assert ic_std >= 0, f"{model_name} ICæ ‡å‡†å·®åº”è¯¥>=0"

    print("\nâœ… æµ‹è¯•2é€šè¿‡!")
    return True


def test_statistical_significance():
    """æµ‹è¯•3: ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•3: ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ")
    print("=" * 60)

    data = create_mock_factor_data(n_samples=500, n_factors=10)
    factor_columns = [col for col in data.columns if col.startswith('factor_')]

    benchmark = MLAlgorithmBenchmark(data, factor_columns, 'return_5d')
    comparison_df = benchmark.run_walk_forward_comparison(n_splits=3)

    # ç»Ÿè®¡æ£€éªŒ
    print("\næ‰§è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ...")
    p_values = benchmark.statistical_significance_test(comparison_df)

    if p_values:
        print("\næ£€éªŒç»“æœ:")
        for name, result in p_values.items():
            print(f"  vs {name}:")
            print(f"    på€¼: {result['p_value']:.4f}")
            print(f"    tç»Ÿè®¡é‡: {result['t_statistic']:.2f}")
            print(f"    æ˜¾è‘—: {result['significant']}")

        # éªŒè¯
        for name, result in p_values.items():
            assert 0 <= result['p_value'] <= 1, "på€¼åº”è¯¥åœ¨[0,1]"
    else:
        print("  åªæœ‰1ä¸ªæ¨¡å‹,æ— éœ€æ£€éªŒ")

    print("\nâœ… æµ‹è¯•3é€šè¿‡!")
    return True


def test_overfitting_check():
    """æµ‹è¯•4: è¿‡æ‹Ÿåˆæ£€æŸ¥"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•4: è¿‡æ‹Ÿåˆæ£€æŸ¥")
    print("=" * 60)

    data = create_mock_factor_data(n_samples=500, n_factors=10)
    factor_columns = [col for col in data.columns if col.startswith('factor_')]

    benchmark = MLAlgorithmBenchmark(data, factor_columns, 'return_5d')
    comparison_df = benchmark.run_walk_forward_comparison(n_splits=3)

    # è¿‡æ‹Ÿåˆæ£€æŸ¥
    print("\næ£€æŸ¥è¿‡æ‹Ÿåˆæƒ…å†µ...")
    overfitting_check = benchmark.check_overfitting(comparison_df)

    print("\nè¿‡æ‹Ÿåˆæ£€æŸ¥ç»“æœ:")
    for name, result in overfitting_check.items():
        print(f"\n  {name}:")
        print(f"    ICå‡å€¼: {result['ICå‡å€¼']:.4f}")
        print(f"    ICæ ‡å‡†å·®: {result['ICæ ‡å‡†å·®']:.4f}")
        print(f"    ç¨³å®šæ€§å¾—åˆ†: {result['ç¨³å®šæ€§å¾—åˆ†']:.2f}")
        print(f"    çŠ¶æ€: {result['çŠ¶æ€']}")
        print(f"    è¯„ä»·: {result['è¯„ä»·']}")

        # éªŒè¯
        assert result['ç¨³å®šæ€§å¾—åˆ†'] >= 0, "ç¨³å®šæ€§å¾—åˆ†åº”è¯¥>=0"

    print("\nâœ… æµ‹è¯•4é€šè¿‡!")
    return True


def test_generate_report():
    """æµ‹è¯•5: ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•5: ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š")
    print("=" * 60)

    data = create_mock_factor_data(n_samples=500, n_factors=10)
    factor_columns = [col for col in data.columns if col.startswith('factor_')]

    benchmark = MLAlgorithmBenchmark(data, factor_columns, 'return_5d')
    comparison_df = benchmark.run_walk_forward_comparison(n_splits=3)
    p_values = benchmark.statistical_significance_test(comparison_df)

    # ç”ŸæˆæŠ¥å‘Š
    print("\nç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
    report = benchmark.generate_report(comparison_df, p_values)

    print("\n" + report)

    # éªŒè¯
    assert len(report) > 0, "æŠ¥å‘Šåº”è¯¥éç©º"
    assert "MLç®—æ³•æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š" in report, "æŠ¥å‘Šåº”è¯¥åŒ…å«æ ‡é¢˜"
    assert "ICå‡å€¼" in report, "æŠ¥å‘Šåº”è¯¥åŒ…å«ICå‡å€¼"

    print("\nâœ… æµ‹è¯•5é€šè¿‡!")
    return True


def test_quick_ml_benchmark():
    """æµ‹è¯•6: å¿«é€Ÿå¯¹æ¯”å‡½æ•°"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•6: å¿«é€Ÿå¯¹æ¯”å‡½æ•°")
    print("=" * 60)

    data = create_mock_factor_data(n_samples=500, n_factors=10)
    factor_columns = [col for col in data.columns if col.startswith('factor_')]

    # ä½¿ç”¨å¿«é€Ÿå‡½æ•°
    print("\nä½¿ç”¨quick_ml_benchmark...")
    result = quick_ml_benchmark(
        data=data,
        factor_columns=factor_columns,
        target_column='return_5d',
        n_splits=3
    )

    print("\nå¿«é€Ÿå¯¹æ¯”å®Œæˆ!")
    print(f"  è¿”å›é”®: {list(result.keys())}")

    # éªŒè¯
    assert 'comparison' in result, "åº”è¯¥è¿”å›comparison"
    assert 'p_values' in result, "åº”è¯¥è¿”å›p_values"
    assert 'overfitting_check' in result, "åº”è¯¥è¿”å›overfitting_check"
    assert 'report' in result, "åº”è¯¥è¿”å›report"

    print("\n" + result['report'])

    print("\nâœ… æµ‹è¯•6é€šè¿‡!")
    return True


if __name__ == "__main__":
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_ml_benchmark_initialization()
        test_walk_forward_comparison()
        test_statistical_significance()
        test_overfitting_check()
        test_generate_report()
        test_quick_ml_benchmark()

        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰Phase 9.1 ML Benchmarkæµ‹è¯•é€šè¿‡!")
        print("MLç®—æ³•å¯¹æ¯”åŠŸèƒ½å·²å‡†å¤‡å°±ç»ª!")
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
